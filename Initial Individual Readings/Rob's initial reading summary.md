## Augmentation aids & accuracy bias
- Mosier, K. L., Skitka, L. J., Heers, S., & Burdick, M. (1998). Automation Bias: Decision Making and Performance in High-Tech Cockpits. _The International Journal of Aviation Psychology_, _8_(1), 47–63. [https://doi.org/10.1207/s15327108ijap0801_3](https://doi.org/10.1207/s15327108ijap0801_3)
	- "**automation bias**, a recently documented factor in the use of automated aids and decision support systems. The term refers to **omission and commission errors resulting from the use of automated cues as a heuristic replacement for vigilant information seeking and processing.**"
	- Automation aids caused pilots to be less attentive to their decisions (including decisions made by aids), unless the pilot had taken steps to internalize a sense of accountability i.e. discipline for retracing their decision-making processes.
	- Interestingly, pilots who blindly followed false signals from their cognitive aid software later 'remembered' seeing corroborating signals with their own senses.
- Skitka, L. J., Mosier, K. L., & Burdick, M. (1999). Does automation bias decision-making? _International Journal of Human-Computer Studies_, _51_(5), 991–1006. [https://doi.org/10.1006/ijhc.1999.0252](https://doi.org/10.1006/ijhc.1999.0252)
	- "Participants in non-automated settings out-performed their counterparts with a very but not perfectly reliable automated aid on a monitoring task. Participants with an aid made errors of omission (missed events when not explicitly prompted about them by the aid) and commission (did what an automated aid recommended, even when it contradicted their training and other 100% valid and available indicators)."
	- Same authors of previous article//very similar content
	- Contains more detail about omission errors, focusing on not-perfectly-reliable cognitive aids. If the aid fails to prompt the user about something it *should*, then the user is very likely to miss that error. Conversely, unlikely to make the same error of omission if there's no aid.
		- "Participants in the non-automated condition responded with 97% accuracy on the six omission error events, whereas participants in the automated condition responded with only a 59% accuracy rate on these same events. People with an AMA were therefore more likely to miss events than those without an AMA, if the AMA failed to notify them of the event." (p 1002)
  - Alon-Barkat, S., & Busuioc, M. (2023). Human–AI Interactions in Public Sector Decision Making: “Automation Bias” and “Selective Adherence” to Algorithmic Advice. _Journal of Public Administration Research and Theory_, _33_(1), 153–169. [https://doi.org/10.1093/jopart/muac007](https://doi.org/10.1093/jopart/muac007)
	  - This article examines bias among public servants in the Netherlands when their decisions are aided by AI models.
	  - The authors theorize two types of bias:
		  1. *automation bias*, where the subject over-relies on advice from an algorithm even when the advice is clearly wrong.
		  2. *selective adherence*, where the subject disproportionately follows advice when it corresponds with their own internal biases/stereotypes.
	  - The authors conducted three experiments. 
		  - In the first, automation bias was tested against similar advice from human experts. Subjects did display automation bias, but in no greater proportion than the same bias provided by the human experts.
		  - In the second experiment, Subjects again displayed selective adherence, but also with no difference in bias rate between machine and human experts.
		  - In the third experiment, both of these biases were tested on real decisions made by real bureaucrats in the Netherlands. The experimental period was directly in the wake of a national scandal that involved selective adherence. The authors found a 'normal' rate of automation bias but no evidence of selective adherence. This suggests that the subjects were paying extra care to this factor since the scandal was top of mind.
 ## Algorithmic bias
 - Kordzadeh, N., & Ghasemaghaei, M. (2022). Algorithmic bias: Review, synthesis, and future research directions. _European Journal of Information Systems_, _31_(3), 388–409. [https://doi.org/10.1080/0960085X.2021.1927212](https://doi.org/10.1080/0960085X.2021.1927212)
	 - This is a lit review of research related to algorithmic bias. Studies were categorized under six themes:
		 1. Ethical, social, and philosophical considerations
		 2. Legal and regulatory implications
		 3. Socio-technical design
		 4. Concerns, perceptions, and needs
		 5. Antecedents of fairness perceptions
		 6. Impacts of machine advice on decisions (which connects with the Mosier, Skitka, et al articles)
	 - "algorithmic bias appears when _an algorithm distributes benefits and burdens unequally among different individuals or groups_." It's important to take a domain-specific approach because one measure of bias can mean a lot in one of the above themes, but have less real impact in another. 
	 - The authors note that algorithmic bias has been canvassed exhaustively at the conceptual level, but little empirical work has been done. "Moreover, the mechanisms through which technology-driven biases translate into decisions and behaviours have been largely overlooked."
	  - The article also highlights the difference between "algorithmic bias, as a computationally-measured construct, and perceived fairness, as a subjective construct."
  ### Algorithmic bias - perceived fairness
   - Wang, R., Harper, F. M., & Zhu, H. (2020). Factors Influencing Perceived Fairness in Algorithmic Decision-Making: Algorithm Outcomes, Development Procedures, and Individual Differences. _Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems_, 1–14. [https://doi.org/10.1145/3313831.3376813](https://doi.org/10.1145/3313831.3376813)
	   - This study stresses that perceptions of fairness are highly complicated, nuanced, and ultimately subjective.
	   - The article examines which factors influence people’s perception of the fairness of algorithmic decisionmaking processes.
	   - "We find that people’s evaluations of fairness are very sensitive to whether or not they receive a positive outcome personally, even surpassing the negative effect of describing an algorithm with strong biases against particular demographic groups."