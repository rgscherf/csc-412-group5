Understanding the role of computer-mediated counter-argument in countering confirmation bias. Hsieh-Hong Huang, Jack Shih-Chieh Hsu, Cheng-Yuan Ku. (2012). Decision Support Systems. (53). 438-447. https:/doi.org/10.1016/j.dss.2012.03.009. 
- It has long been established that confirmation bias can affect decision-making. The authors propose examination of a unique decisional characteristic – decision confidence – a subject’s beliefs about their decisional performance and their perception of the probability that their decisions are correct.
- The authors hypothesize that the presence of confirmation bias includes abnormally high decision confidence and inadequate adjustment of decisions. 
- They attempt to block or otherwise reduce confirmation bias through an ‘embedded de-bias function’ called computer mediated counter-argument. The DSS provides counter-argument according to the decision maker’s information reading behaviour.
- 241 subjects recruited from students enrolled in MBA programs at 7 universities. Divided into two groups – one with DSS counter-argument, the other without. They were given a stock investment task asking them to invest X amount between two companies with the decision process spanning an initial decision, exposure to a broad overview of relevant market data (that they would then sift through themselves), and a final decision. At each stage, their decision confidence and satisfaction with the process were measured.
- Findings included support for hypotheses claiming confirmation bias has less effect on both decision confidence and decision adjustment with computer-mediated counter-argument than for those without it, as well as subjects who use DSS with computer-mediated counter-argument will feel more satisfied with the decision-making outcome.

How to Use AI to Eliminate Bias Glenn Gow. (2022). Retrieved from: [https://www.forbes.com/sites/glenngow/2022/07/17/how-to-use-ai-to-eliminate-bias/?sh=67b83c0c1f1f](https://www.forbes.com/sites/glenngow/2022/07/17/how-to-use-ai-to-eliminate-bias/?sh=67b83c0c1f1f).

- Admittedly, this is more of a general article about the presence of bias in AI. Some initial commentary about whether AI reduces or worsens bias in decision-making before pivoting towards argument that it does harbor potential for reducing AI across various industries.
- I thought this article may be helpful in the sense that it offers several real-world examples of software tools in use to identify and eliminate AI in hiring, venture capitalism, healthcare, etc. I think A3 calls for some discussion about real-world examples so I can perform a deeper dive on the technologies listed if that’s something we want to include. 

The Impact of Computer-Mediated Communications Systems on Biased Group Discussion. Hightower, Ross and Sayeed, Luftus. (1995). Computers in Human Behaviour. (11)1, 33-44. https://doi.org/10.1016/0747-5632(94)00019-E
- Study with ~90 undergraduates performed to examine the effects of communication mode (F2F vs. remote), information load, and distribution of information on biased discussion – the tendency for group members to spend more time and energy discussing information the group is already familiar with. They were tasked with settling on a hiring candidate through group discussion given information provided to individuals in the group. The amount of information shared between group individuals prior to discussion ranged from 33% to 60%. 90% of the groups chose the wrong candidate suggesting biased discussion occurred.
- Communication Mode – Results found CMCS discussions had >2x likelihood of resulting in biased discussion vs. the F2F counterpart.
- Information Distribution – Irrespective of communication mode, the greater the information distribution in the provided info to group members prior to group discussion, the greater the biased discussion. For groups with 60% distribution, the degree of biased discussion was more than twice that of 33% distribution.
- Information Load – It was not supported that higher information load will lead to greater discussion bias regardless of communication. This connection between information load and greater discussion bias was however supported for those groups required to communicate in CMCS vs. F2F. 

The Bias in the Machine: Facial Recognition Technology and Racial Disparities. Sidney Perkowitz. (2021). MIT Schwarzman College of Computing. Retrieved from: [https://mit-serc.pubpub.org/pub/bias-in-machine/release/1?readingCollection=34db8026](https://mit-serc.pubpub.org/pub/bias-in-machine/release/1?readingCollection=34db8026).
- Growing research suggests FRTs are guilty of troubling levels of bias, particularly as these technologies are incorporated into law enforcement agencies. As we all struggle to contain these issues, the author claims that we can turn to older forensic technologies, such as fingerprint ID, for answers. 
- Fingerprint ID was prone to egregious errors as well, including it’s use to wrongfully ID suspects in the 2004 Madrid terrorist bombings of commuter trains.
- In 2009 and 2016 concerted efforts were made on the federal level to scrutinize this technology and implement overarching reforms to control the use fingerprint ID and confirm the veracity of the results.
- No such approaches have been implemented regarding facial recognition. Municipal law enforcement agencies are free to choose among scores of commercially available algorithms without clear guidance about their use or reliability.

Johansen, J., Pedersen, T. & Johansen, C. Studying human-to-computer bias transference. AI & Soc 38, 1659–1683 (2023). [https://doi.org/10.1007/s00146-021-01328-4](https://doi.org/10.1007/s00146-021-01328-4)
- Building off the understanding that bias is frequently transferred in machine learning from the data set that a learning algorithm is trained upon, the authors expand on this by examining whether programmers’ cultural background, including education and line of work, as well as the contextual programming environment, including software requirements or developer tools, also contribute to bias transference. 
- Cultural Background – The authors observed that cultural background influences choices made and are subsequently transferred from the programmer to the program artifact. Cultural metaphors in terms of irrelevant and inappropriate influences on the programming task at hand represent instances of biases that are being transferred from humans to machines.
- The transference of bias to programming products is even more likely when conditions of uncertainty exist – as we all know, this is often the case in systems programming.