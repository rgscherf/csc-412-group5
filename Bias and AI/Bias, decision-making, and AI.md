## Overcoming financial planners’ cognitive biases through digitalization: A qualitative study
### Vidya S. Athota, Vijay Pereira, Zahid Hasan, Daicy Vaz, Benjamin Laker, Dimitrios Reppas
https://www.sciencedirect.com/science/article/pii/S0148296322007445
- "The purpose of this paper is to investigate cognitive biases among financial planners and, if and how, digital transformation through Artificial Intelligence (AI) can help overcome biases." #practitioner-contexts 
- results found that financial planners do struggle with cognitive biases when providing services. "digital transformation" using A.I. was shown to possibly help overcome the existing biases, when paired with the individuals own cognitive intelligence. 

## Unconscious Other’s Impression Changer: A Method to Manipulate Cognitive Biases That Subtly Change Others’ Impressions Positively/Negatively by Making AI Bias in Emotion Estimation AI
### Futami K, Yanase S, Murao K, Terada T
https://doi.org/10.3390/s22249961
- Using emotional estimation AI has shown to manipulate peoples' perceptions of others, which can be considered for altering cognitive biases. 
- " Viewing information that estimated others’ emotions more positively/negatively caused the phenomenon in which the user’s self-judgment was overridden and others’ impressions of emotions, words, and actions were perceived more positively/negatively." #perception 

## Human–AI Interactions in Public Sector Decision Making: “Automation Bias” and “Selective Adherence” to Algorithmic Advice
### Alon-Barkat, S., & Busuioc, M.
https://doi.org/10.1093/jopart/muac007
- This article examines bias among public servants in the Netherlands when their decisions are aided by AI models. #practitioner-contexts 
- The authors theorize two types of bias:
	1. #automation-bias, where the subject over-relies on advice from an algorithm even when the advice is clearly wrong.
	2. *selective adherence*, where the subject disproportionately follows advice when it corresponds with their own internal biases/stereotypes.
- The authors conducted three experiments. 
- In the first, automation bias was tested against similar advice from human experts. Subjects did display automation bias, but in no greater proportion than the same bias provided by the human experts.
- In the second experiment, Subjects again displayed selective adherence, but also with no difference in bias rate between machine and human experts.
- In the third experiment, both of these biases were tested on real decisions made by real bureaucrats in the Netherlands. The experimental period was directly in the wake of a national scandal that involved selective adherence. The authors found a 'normal' rate of automation bias but no evidence of selective adherence. This suggests that the subjects were paying extra care to this factor since the scandal was top of mind.
- Unstated in the paper is that bureaucratic decisions are also #group decisions!

## Studying human-to-computer bias transference
### Johansen, J., Pedersen, T. & Johansen, C
https://doi.org/10.1007/s00146-021-01328-4
- Building off the understanding that bias is frequently transferred in machine learning from the data set that a learning algorithm is trained upon, the authors expand on this by examining whether programmers’ cultural background, including education and line of work, as well as the contextual programming environment and #group dynamics, including software requirements or developer tools, also contribute to bias transference. 
- Cultural Background – The authors observed that cultural background influences choices made and are subsequently transferred from the programmer to the program artifact. Cultural metaphors in terms of irrelevant and inappropriate influences on the programming task at hand represent instances of biases that are being transferred from humans to machines.
- The transference of bias to programming products is even more likely when conditions of uncertainty exist – as we all know, this is often the case in systems programming.

## How to Use AI to Eliminate Bias
### Glenn Gow
Retrieved from: https://www.forbes.com/sites/glenngow/2022/07/17/how-to-use-ai-to-eliminate-bias/?sh=67b83c0c1f1f
- Admittedly, this is more of a general article about the presence of bias in AI. Some initial commentary about whether AI reduces or worsens bias in decision-making before pivoting towards argument that it does harbor potential for reducing AI across various industries.
- Real-world examples of software tools in use to identify and eliminate AI in hiring, venture capitalism, healthcare, etc. These include Textio, Gender Decoder, and Ongig which scrutinize job postings for hidden biases around gender. Knockri, Ceridian, and Gapjumpers use AI to remove or ignore characteristics that identify gender, national origin, skin colour, etc so that hiring managers can focus on the professional qualifications pertinent to the open position.
- Google's What-If tool helps developers assess bias in AI while PWC's Bias Analyzer and IBM's AI Fairness 360 each help identify the existence of bias in your AI code.
